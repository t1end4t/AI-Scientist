[
  {
    "Name": "adaptive_block_size",
    "Title": "Adaptive Block Size: Dynamic Context Window Adjustment for Efficient Training",
    "Experiment": "Modify the model to dynamically adjust its block size during training, starting with a smaller block size and gradually increasing it. This could potentially lead to faster initial training and better long-range dependency learning.",
    "Interestingness": 6,
    "Feasibility": 4,
    "Novelty": 4,
    "novel": true
  },
  {
    "Name": "layerwise_learning_rates",
    "Title": "Layer-wise Learning Rate Adaptation: Optimizing Training Dynamics in Transformer Models",
    "Experiment": "Implement layer-wise learning rates, where each transformer layer has its own learning rate. Modify the configure_optimizers function to assign different learning rates to different layers, with deeper layers having lower learning rates. Compare the training dynamics, convergence speed, and final performance with the baseline model.",
    "Interestingness": 4,
    "Feasibility": 6,
    "Novelty": 2,
    "novel": true
  },
  {
    "Name": "initialization_schemes_model_size",
    "Title": "Impact of Initialization Schemes and Model Size on Small Language Model Training",
    "Experiment": "Modify the `_init_weights` function in the `GPT` class to support different initialization schemes: standard normal (current), orthogonal, and Xavier/Glorot initialization. Add a `initialization` parameter to the GPTConfig dataclass. Run experiments on shakespeare_char, enwik8, and text8 datasets with multiple seeds, varying both the initialization scheme and the model size (n_layer, n_head, n_embd). Compare training loss, validation loss, final generation quality (using the existing sampling script), and training time for each combination of initialization scheme and model size. Analyze the results to determine the interaction between initialization, model capacity, and generalization performance.",
    "Interestingness": 8,
    "Feasibility": 6,
    "Novelty": 7,
    "novel": true
  },
  {
    "Name": "activation_regularization",
    "Title": "Enhancing Generalization in Small Language Models through Activation Regularization",
    "Experiment": "Modify the training loop in `experiment.py` to incorporate L1 and L2 regularization on the activations of the final hidden layer (output of ln_f in GPT). Add `l1_reg` and `l2_reg` parameters to the GPTConfig dataclass, defaulting to 0.0. Within the training loop, after the final layer norm's forward pass (`x = self.transformer.ln_f(x)`), compute the L1 and L2 norms of the activations (`x`) and add them to the loss, scaled by `l1_reg` and `l2_reg` respectively. Run experiments on shakespeare_char, enwik8, and text8 datasets with multiple seeds, varying the values of `l1_reg` and `l2_reg`. Compare training loss, validation loss, final generation quality (using the existing sampling script), and training time for each combination of regularization strengths. Analyze the results to determine the impact of activation regularization on generalization performance.",
    "Interestingness": 7,
    "Feasibility": 8,
    "Novelty": 6,
    "novel": true
  },
  {
    "Name": "relative_positional_embeddings",
    "Title": "Exploring Relative Positional Embeddings for Enhanced Generalization in Small Language Models",
    "Experiment": "Implement a relative positional embedding scheme in the GPT model. This involves the following steps:\n1.  Add a `positional_embedding_type` parameter to the `GPTConfig` dataclass, with options 'absolute' (default) and 'relative'.\n2.  Modify the `__init__` function of the `GPT` class to conditionally initialize either absolute or relative positional embeddings based on the `positional_embedding_type`.\n    *   For absolute embeddings, keep the existing `wpe` embedding layer.\n    *   For relative embeddings, create a new embedding layer `rpe` that maps relative distances to embedding vectors. The size of the relative distance embedding should be twice the block size minus one (2*block_size - 1), to account for distances from -block_size+1 to block_size-1.\n3.  Modify the `forward` function of the `CausalSelfAttention` class to incorporate relative positional information.\n    *   If `positional_embedding_type` is 'relative', calculate the relative distances between tokens in the sequence.\n    *   Use these distances to index into the `rpe` embedding layer to obtain relative positional embeddings.\n    *   Incorporate the relative positional embeddings into the attention mechanism by adding them to the attention scores (before softmax). This can be done by adding a learned scalar weight to the positional embeddings before they are added to attention scores.\n4.  Run experiments on shakespeare_char, enwik8, and text8 datasets with multiple seeds, comparing the performance of absolute and relative positional embeddings. Evaluate training loss, validation loss, and generation quality (using the existing sampling script). Analyze the results to determine the impact of relative positional embeddings on generalization, especially for sequences longer than the training block size. Also measure the training time for each positional embedding type.\n5. Add logic to save and load the positional embedding type into the checkpoint.",
    "Interestingness": 8,
    "Feasibility": 6,
    "Novelty": 7,
    "novel": true
  },
  {
    "Name": "rms_norm",
    "Title": "Investigating RMSNorm for Efficient Training of Small Language Models",
    "Experiment": "Implement RMSNorm as an alternative to LayerNorm in the GPT model. 1. Add a `norm_type` parameter to the `GPTConfig` dataclass, with options 'layernorm' (default) and 'rmsnorm'. 2. Implement the RMSNorm class in `model.py`. 3. Modify the `LayerNorm` instantiation in the `Block` class to conditionally instantiate either LayerNorm or RMSNorm based on the `norm_type` parameter. 4. Add a `disable_bias` parameter to `GPTConfig`. Modify LayerNorm and RMSNorm implementations to allow disabling the bias term. 5. Run experiments on shakespeare_char, enwik8, and text8 datasets with multiple seeds, comparing the performance of LayerNorm and RMSNorm with and without bias. Evaluate training loss, validation loss, generation quality (using the existing sampling script), training time per iteration, and memory usage for each normalization technique. Analyze the results to determine the impact of RMSNorm on training efficiency and generalization performance. 6. Add logic to save and load the norm type and disable_bias flag into the checkpoint.",
    "Interestingness": 8,
    "Feasibility": 7,
    "Novelty": 6,
    "novel": true
  },
  {
    "Name": "alternative_attention_mechanisms",
    "Title": "Exploring Alternative Attention Mechanisms for Efficient Small Language Models",
    "Experiment": "Implement and compare different attention mechanisms within the GPT model. Add an `attention_type` parameter to the `GPTConfig` dataclass, with options 'scaled_dot_product' (default), 'linear', and 'block_sparse'.\n\n1. Implement Linear Attention: Modify the `CausalSelfAttention` class to support linear attention. This involves replacing the dot product attention with a linear kernel function. The linear kernel can be implemented as `Q @ K.transpose(-1, -2) = (Q @ W) @ (K @ W).transpose(-1, -2)`, where `W` is a learnable projection matrix. Ensure the number of parameters is similar to the original attention by adjusting the dimension of W.\n2. Implement Block Sparse Attention: Implement a block sparse attention, where the attention matrix is divided into blocks and only a fixed, strided subset of blocks is computed. The block size should be a hyperparameter. \n3. Modify the `CausalSelfAttention` class to conditionally use the selected attention mechanism based on the `attention_type` parameter.\n4. Run experiments on shakespeare_char, enwik8, and text8 datasets with multiple seeds, comparing the performance of different attention mechanisms. Evaluate training loss, validation loss, generation quality (using the existing sampling script), training time per iteration, and memory usage for each attention mechanism. Analyze the results to determine the impact of different attention mechanisms on training efficiency and generalization performance. Keep the number of parameters roughly constant across different attention types.\n5. Add logic to save and load the attention type and block size (if applicable) into the checkpoint.",
    "Interestingness": 9,
    "Feasibility": 6,
    "Novelty": 7,
    "novel": true
  }
]
